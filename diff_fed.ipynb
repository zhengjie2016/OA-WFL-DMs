{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:19.482059Z",
     "start_time": "2024-09-30T05:06:19.463058Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:20.108910Z",
     "start_time": "2024-09-30T05:06:20.103909Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:20.668186Z",
     "start_time": "2024-09-30T05:06:20.649190Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:21.362013Z",
     "start_time": "2024-09-30T05:06:21.349833Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:23.161513Z",
     "start_time": "2024-09-30T05:06:23.142512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom files.\n",
    "from model import DiffusionModel\n",
    "from training import get_config, ImgDataset, evaluate_model, demo, generate_img\n",
    "from diffusion import diffusion_loss\n",
    "import federated_learning as fl\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:25.992317Z",
     "start_time": "2024-09-30T05:06:25.982318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset and settings.\n",
    "# dataset_name = 'cifar_10'\n",
    "# dataset_name = 'svhn'\n",
    "dataset_name = 'fashion_mnist'\n",
    "\n",
    "# Define distribution settings.\n",
    "num_clients = 10      # [10, 30, 50]\n",
    "beta = 0.5            # [0.1, 0.5, 5]\n",
    "\n",
    "if num_clients == 10:\n",
    "    num_participation = 10\n",
    "elif num_clients == 30:\n",
    "    num_participation = 10\n",
    "elif num_clients == 50:\n",
    "    num_participation = 10\n",
    "client_idxes = list(range(num_clients))\n",
    "\n",
    "client_data_dir = os.path.join('./client_data/', dataset_name + '_b_{}_c_{}'.format(beta, num_clients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:27.089700Z",
     "start_time": "2024-09-30T05:06:27.068700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_size': 32, 'channels': 3, 'batch_size': 512, 'train_transform': Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      "), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")}\n",
      "{'lr': 0.0002, 'timesteps': 300, 'epochs': 10, 'rounds': 2, 'local_epochs': 5, 'ema_decay': 0.998, 'eta': 1.0, 'save_interval': 100}\n"
     ]
    }
   ],
   "source": [
    "data_config, train_config = get_config(dataset_name)\n",
    "print(data_config)\n",
    "print(train_config)\n",
    "\n",
    "# Data config.\n",
    "# img_size = data_config['img_size']\n",
    "# channels = data_config['channels']\n",
    "batch_size = data_config['batch_size']\n",
    "train_transform = data_config['train_transform']\n",
    "test_transform = data_config['test_transform']\n",
    "\n",
    "# Training config.\n",
    "lr = train_config['lr']\n",
    "timesteps = train_config['timesteps']\n",
    "num_rounds = train_config['rounds']\n",
    "num_local_epochs = train_config['local_epochs']\n",
    "eta = train_config['eta']\n",
    "save_interval = train_config['save_interval']\n",
    "\n",
    "# num_local_epochs = 1               # 1, 5, 10, 15\n",
    "\n",
    "save_dir = os.path.join('./output/diffusion_fedavg', dataset_name + '_b_{}_c_{}_le_{}'.format(beta, num_clients, num_local_epochs))\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:30.049587Z",
     "start_time": "2024-09-30T05:06:29.957588Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImgDataset' object has no attribute 'img_list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Centralized testset for global model evaluation.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m test_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m test_data \u001B[38;5;241m=\u001B[39m \u001B[43mImgDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(test_data, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, persistent_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mE:\\LSS\\federated_diffusion_model-main\\training.py:65\u001B[0m, in \u001B[0;36mImgDataset.__init__\u001B[1;34m(self, parent_dir, label_idx_dict, transform)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, parent_dir, label_idx_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal images loaded: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_list)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ImgDataset' object has no attribute 'img_list'"
     ]
    }
   ],
   "source": [
    "# Data.\n",
    "data_dir = os.path.join('../datasets/', dataset_name)\n",
    "\n",
    "# Centralized testset for global model evaluation.\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "test_data = ImgDataset(test_dir, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:32.081748Z",
     "start_time": "2024-09-30T05:06:32.035747Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImgDataset' object has no attribute 'img_list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_clients):\n\u001B[0;32m      4\u001B[0m     data_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(client_data_dir, \u001B[38;5;28mstr\u001B[39m(client_idx))\n\u001B[1;32m----> 5\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mImgDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, persistent_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m     client_loaders\u001B[38;5;241m.\u001B[39mappend(data_loader)\n",
      "File \u001B[1;32mE:\\LSS\\federated_diffusion_model-main\\training.py:65\u001B[0m, in \u001B[0;36mImgDataset.__init__\u001B[1;34m(self, parent_dir, label_idx_dict, transform)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, parent_dir, label_idx_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal images loaded: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_list)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ImgDataset' object has no attribute 'img_list'"
     ]
    }
   ],
   "source": [
    "# Define client data.\n",
    "client_loaders = []\n",
    "for client_idx in range(num_clients):\n",
    "    data_dir = os.path.join(client_data_dir, str(client_idx))\n",
    "    dataset = ImgDataset(data_dir, transform=train_transform)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True, pin_memory=True)\n",
    "    client_loaders.append(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:33.637202Z",
     "start_time": "2024-09-30T05:06:33.606466Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m client_loader \u001B[38;5;241m=\u001B[39m \u001B[43mclient_loaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Useful for image generation later. Just to check which images are generated.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m label_idx_dict \u001B[38;5;241m=\u001B[39m client_loader\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mlabel_idx_dict\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "client_loader = client_loaders[0]\n",
    "\n",
    "# Useful for image generation later. Just to check which images are generated.\n",
    "label_idx_dict = client_loader.dataset.label_idx_dict\n",
    "idx_label_dict = {idx: label for label, idx in label_idx_dict.items()}\n",
    "print(idx_label_dict)\n",
    "\n",
    "num_classes = len(idx_label_dict)\n",
    "\n",
    "# Print a few images.\n",
    "dataiter = iter(client_loader)\n",
    "images, labels = next(dataiter)\n",
    "utils.show_img_tensor(torchvision.utils.make_grid(images[:32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:35.121164Z",
     "start_time": "2024-09-30T05:06:34.946131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global model.\n",
    "glob_model = DiffusionModel().to(device)\n",
    "\n",
    "# For logging model performance.\n",
    "performance_dict, performance_log = dict(), dict()\n",
    "metric_keys = ['g_train_loss', 'g_test_loss']\n",
    "for metric in metric_keys:\n",
    "    performance_dict[metric] = utils.MeanMetric()\n",
    "    performance_log[metric] = list()\n",
    "\n",
    "# performance_dict, performance_log = utils.get_performance_loggers(metric_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:36.000672Z",
     "start_time": "2024-09-30T05:06:35.991672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Automatic resuming from checkpoint.\n",
    "log_path = os.path.join(save_dir, 'performance_log.pickle')\n",
    "if os.path.isfile(log_path):\n",
    "    performance_log = utils.load_pickle(log_path)\n",
    "start_round = len(performance_log[metric_keys[0]])\n",
    "\n",
    "if start_round > 0:\n",
    "    glob_model.load_state_dict(torch.load(os.path.join(save_dir, 'g_r_{}.pth'.format(start_round))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:37.028318Z",
     "start_time": "2024-09-30T05:06:36.962322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Round: 1 / 2 ====================\n",
      "\n",
      "participating_clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "client: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_idx \u001B[38;5;129;01min\u001B[39;00m participating_clients:  \n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclient:\u001B[39m\u001B[38;5;124m'\u001B[39m, client_idx)\n\u001B[1;32m---> 16\u001B[0m     client_loader \u001B[38;5;241m=\u001B[39m \u001B[43mclient_loaders\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclient_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Local training.\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     client_update \u001B[38;5;241m=\u001B[39m fl\u001B[38;5;241m.\u001B[39mlocal_update_fedavg(glob_model, client_loader, num_local_epochs, optim_args, test_loader\u001B[38;5;241m=\u001B[39mtest_loader)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "glob_w = glob_model.state_dict()\n",
    "\n",
    "optim_args = {'lr': lr}\n",
    "\n",
    "for round_no in range(start_round, num_rounds):\n",
    "    utils.print_separator(text='Round: {} / {}'.format(round_no + 1, num_rounds))\n",
    "                \n",
    "    participating_clients = sorted(np.random.choice(client_idxes, size=num_participation, replace=False))\n",
    "    print('participating_clients:', participating_clients)\n",
    "    \n",
    "    # Local training.\n",
    "    glob_model.train()\n",
    "    client_updates = dict()\n",
    "    for client_idx in participating_clients:  \n",
    "        print('client:', client_idx)\n",
    "        client_loader = client_loaders[client_idx]\n",
    "        \n",
    "        # Local training.\n",
    "        client_update = fl.local_update_fedavg(glob_model, client_loader, num_local_epochs, optim_args, test_loader=test_loader)\n",
    "        \n",
    "        # For weighted averaging.\n",
    "        client_updates.setdefault('local_w', list()).append(client_update['local_w'])\n",
    "        client_updates.setdefault('num_samples', list()).append(client_update['num_samples'])\n",
    "        \n",
    "        # Logging client performance.\n",
    "        performance_log.setdefault('c_{}_train_loss'.format(client_idx), list()).append(client_update['train_loss'])\n",
    "        performance_log.setdefault('c_{}_test_loss'.format(client_idx), list()).append(client_update['test_loss'])\n",
    "            \n",
    "    # Model aggregation.\n",
    "    glob_w = fl.weighted_averaging(client_updates['local_w'], client_updates['num_samples'])\n",
    "    glob_model.load_state_dict(glob_w)\n",
    "    \n",
    "    # Evaluate global model on local datasets.\n",
    "    for client_idx in participating_clients:\n",
    "        g_train_loss = evaluate_model(glob_model, client_loader, diffusion_loss, tqdm_desc='g_train_loss')\n",
    "        performance_dict['g_train_loss'].update_state(g_train_loss)\n",
    "        \n",
    "    # Average local performace of global model.\n",
    "    performance_log['g_train_loss'].append(performance_dict['g_train_loss'].result())\n",
    "    performance_dict['g_train_loss'].reset_state()\n",
    "    \n",
    "    # Evaluate global model on global testset.\n",
    "    g_test_loss = evaluate_model(glob_model, test_loader, diffusion_loss, tqdm_desc='g_test_loss')\n",
    "    performance_log['g_test_loss'].append(g_test_loss)\n",
    "    \n",
    "#     for key in sorted(performance_log.keys()):\n",
    "#         print(key, ': ',  performance_log[key][-1])\n",
    "    \n",
    "    for key in sorted(metric_keys):\n",
    "        print(key, ': ',  performance_log[key][-1])\n",
    "    \n",
    "    # Save global model.\n",
    "    if (round_no + 1) % save_interval == 0:\n",
    "        torch.save(glob_model.state_dict(), os.path.join(save_dir, 'g_r_{}.pth'.format(round_no + 1)))\n",
    "        utils.save_pickle(log_path, performance_log)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Save client models only in the last round. To save space.\n",
    "#     if round_no + 1 == num_rounds:\n",
    "#         for client_idx in range(num_clients):\n",
    "#             local_state_dict = client_updates['local_w'][client_idx]\n",
    "#             torch.save(local_state_dict, os.path.join(save_dir, 'c_{}_r_{}.pth'.format(client_idx, round_no + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:39.482145Z",
     "start_time": "2024-09-30T05:06:39.438145Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\performance_log.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Plot training history.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m performance_log \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_pickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m loss_plot_config \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mLOSS_PLOT_CONFIG\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      5\u001B[0m loss_plot_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msave_dir\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_loss.png\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\LSS\\federated_diffusion_model-main\\utils.py:21\u001B[0m, in \u001B[0;36mload_pickle\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_pickle\u001B[39m(file_path):\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     22\u001B[0m         obj \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\performance_log.pickle'"
     ]
    }
   ],
   "source": [
    "# Plot training history.\n",
    "performance_log = utils.load_pickle(log_path)\n",
    "\n",
    "loss_plot_config = utils.LOSS_PLOT_CONFIG.copy()\n",
    "loss_plot_config['save_dir'] = os.path.join(save_dir, 'test_loss.png')\n",
    "loss_plot_config['show_img'] = True\n",
    "loss_plot_config['xlabel'] = 'rounds'\n",
    "loss_plot_config['labels'] = ['g_test_loss'] + ['c_{}_test_loss'.format(client_idx) for client_idx in range(num_clients)]\n",
    "loss_plot_config['attributes'] = ['g_test_loss'] + ['c_{}_test_loss'.format(client_idx) for client_idx in range(num_clients)]\n",
    "utils.save_history_plot(performance_log, loss_plot_config)\n",
    "\n",
    "loss_plot_config = utils.LOSS_PLOT_CONFIG.copy()\n",
    "loss_plot_config['save_dir'] = os.path.join(save_dir, 'train_loss.png')\n",
    "loss_plot_config['show_img'] = True\n",
    "loss_plot_config['xlabel'] = 'rounds'\n",
    "loss_plot_config['labels'] = ['g_train_loss'] + ['c_{}_train_loss'.format(client_idx) for client_idx in range(num_clients)]\n",
    "loss_plot_config['attributes'] = ['g_train_loss'] + ['c_{}_train_loss'.format(client_idx) for client_idx in range(num_clients)]\n",
    "utils.save_history_plot(performance_log, loss_plot_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:42.736188Z",
     "start_time": "2024-09-30T05:06:42.324785Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\g_r_2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reload saved global model for generation.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m glob_model \u001B[38;5;241m=\u001B[39m DiffusionModel()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 3\u001B[0m glob_model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mg_r_\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      5\u001B[0m img_folder \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(img_folder, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\g_r_2.pth'"
     ]
    }
   ],
   "source": [
    "# Reload saved global model for generation.\n",
    "glob_model = DiffusionModel().to(device)\n",
    "glob_model.load_state_dict(torch.load(os.path.join(save_dir, 'g_r_{}.pth'.format(num_rounds))))\n",
    "\n",
    "img_folder = os.path.join(save_dir, 'images')\n",
    "os.makedirs(img_folder, exist_ok=True)\n",
    "\n",
    "# Generate and save image.\n",
    "img_path = os.path.join(img_folder, 'g_final.png')\n",
    "# demo(glob_model, img_path, timesteps, eta)\n",
    "demo(glob_model, img_path, timesteps, eta, num_images=4, seed=0)\n",
    "# demo(glob_model, img_path, timesteps, eta, num_images=40, classes=[9], num_img_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:44.399205Z",
     "start_time": "2024-09-30T05:06:43.962954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\c_0_r_2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_clients):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(client_idx)\n\u001B[1;32m----> 5\u001B[0m     local_model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mc_\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m_r_\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclient_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      6\u001B[0m     img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(img_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mc_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m_final.png\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(client_idx))\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#     demo(local_model, img_path, timesteps, eta)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\lss666\\lib\\site-packages\\torch\\serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './output/diffusion_fedavg\\\\fashion_mnist_b_0.5_c_10_le_5\\\\c_0_r_2.pth'"
     ]
    }
   ],
   "source": [
    "# Reload saved client models for generation.\n",
    "local_model = DiffusionModel().to(device)\n",
    "for client_idx in range(num_clients):\n",
    "    print(client_idx)\n",
    "    local_model.load_state_dict(torch.load(os.path.join(save_dir, 'c_{}_r_{}.pth'.format(client_idx, num_rounds))))\n",
    "    img_path = os.path.join(img_folder, 'c_{}_final.png'.format(client_idx))\n",
    "#     demo(local_model, img_path, timesteps, eta)\n",
    "    demo(local_model, img_path, timesteps, eta, num_images=15, classes=[4], num_img_per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T05:06:45.377562Z",
     "start_time": "2024-09-30T05:06:45.366562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate images for evaluation.\n",
    "# idx_label_dict = {v: k for k, v in test_data.label_idx_dict.items()}\n",
    "# for idx, label in idx_label_dict.items():\n",
    "#     num_imgs = test_data.label_count_dict[label]\n",
    "#     img_class = idx\n",
    "#     class_dir = os.path.join(save_dir, 'generated_img', label)\n",
    "#     os.makedirs(class_dir, exist_ok=True)\n",
    "#     generate_img(glob_model, class_dir, timesteps, eta, num_imgs, img_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T12:34:42.552637Z",
     "start_time": "2024-09-29T12:34:42.537637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<script>Jupyter.notebook.kernel.restart()</script>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
